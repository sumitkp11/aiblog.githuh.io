<!doctype html>
<html lang="en">
	<head>
	<link rel="icon" type="image/x-icon" href="./favicon.ico">
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Fine-tune a Pre-trained model for Custom Domain</title>
		<meta name="description" content="Let’s see the following points covered in the course provided by Nvidia&#39;s Deep Learning Institue on Transformers-based Natural Language Processing">
		<link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="The Sumit&#39;s AI Blog">
		
		
		
		<style>/* This is an arbitrary CSS string added to the bundle */
/* Defaults */
:root {
	--font-family: -apple-system, system-ui, sans-serif;
	--font-family-monospace: Consolas, Menlo, Monaco, Andale Mono WT, Andale Mono, Lucida Console, Lucida Sans Typewriter, DejaVu Sans Mono, Bitstream Vera Sans Mono, Liberation Mono, Nimbus Mono L, Courier New, Courier, monospace;
}

/* Theme colors */
:root {
	--color-gray-20: #e0e0e0;
	--color-gray-50: #C0C0C0;
	--color-gray-90: #333;

	--background-color: #fff;

	--text-color: var(--color-gray-90);
	--text-color-link: #082840;
	--text-color-link-active: #5f2b48;
	--text-color-link-visited: #17050F;

	--syntax-tab-size: 2;
}

@media (prefers-color-scheme: dark) {
	:root {
		--color-gray-20: #e0e0e0;
		--color-gray-50: #C0C0C0;
		--color-gray-90: #dad8d8;

		/* --text-color is assigned to --color-gray-_ above */
		--text-color-link: #1493fb;
		--text-color-link-active: #6969f7;
		--text-color-link-visited: #a6a6f8;

		--background-color: #15202b;
	}
}


/* Global stylesheet */
* {
	box-sizing: border-box;
}

@view-transition {
	navigation: auto;
}

html,
body {
	padding: 0;
	margin: 0 auto;
	font-family: var(--font-family);
	color: var(--text-color);
	background-color: var(--background-color);
}
html {
	overflow-y: scroll;
}
body {
	max-width: 40em;
}

/* https://www.a11yproject.com/posts/how-to-hide-content/ */
.visually-hidden:not(:focus):not(:active) {
	clip: rect(0 0 0 0);
	clip-path: inset(50%);
	height: 1px;
	overflow: hidden;
	position: absolute;
	white-space: nowrap;
	width: 1px;
}

/* Fluid images via https://www.zachleat.com/web/fluid-images/ */
img{
  max-width: 100%;
}
img[width][height] {
  height: auto;
}
img[src$=".svg"] {
  width: 100%;
  height: auto;
  max-width: none;
}
video,
iframe {
	width: 100%;
	height: auto;
}
iframe {
	aspect-ratio: 16/9;
}

p:last-child {
	margin-bottom: 0;
}
p {
	line-height: 1.5;
}

li {
	line-height: 1.5;
}

a[href] {
	color: var(--text-color-link);
}
a[href]:visited {
	color: var(--text-color-link-visited);
}
a[href]:hover,
a[href]:active {
	color: var(--text-color-link-active);
}

main,
footer {
	padding: 1rem;
}
main :first-child {
	margin-top: 0;
}

header {
	border-bottom: 1px dashed var(--color-gray-20);
}

#skip-link {
	text-decoration: none;
	background: var(--background-color);
	color: var(--text-color);
	padding: 0.5rem 1rem;
	border: 1px solid var(--color-gray-90);
	border-radius: 2px;
}

/* Prevent visually-hidden skip link fom pushing content around when focused */
#skip-link.visually-hidden:focus {
	position: absolute;
	top: 1rem;
	left: 1rem;
	/* Ensure it is positioned on top of everything else when it is shown */
	z-index: 999;
}

.links-nextprev {
	display: flex;
	justify-content: space-between;
	gap: .5em 1em;
	list-style: "";
	border-top: 1px dashed var(--color-gray-20);
	padding: 1em 0;
}
.links-nextprev > * {
	flex-grow: 1;
}
.links-nextprev-next {
	text-align: right;
}

table {
	margin: 1em 0;
}
table td,
table th {
	padding-right: 1em;
}

pre,
code {
	font-family: var(--font-family-monospace);
}
pre:not([class*="language-"]) {
	margin: .5em 0;
	line-height: 1.375; /* 22px /16 */
	-moz-tab-size: var(--syntax-tab-size);
	-o-tab-size: var(--syntax-tab-size);
	tab-size: var(--syntax-tab-size);
	-webkit-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
	direction: ltr;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	overflow-x: auto;
}
code {
	word-break: break-all;
}

/* Header */
header {
	display: flex;
	gap: 1em;
	flex-wrap: wrap;
	justify-content: space-between;
	align-items: center;
	padding: 1em;
}
.home-link {
	flex-grow: 1;
	font-size: 1em; /* 16px /16 */
	font-weight: 700;
}
.home-link:link:not(:hover) {
	text-decoration: none;
}

/* Nav */
.nav {
	display: flex;
	gap: .5em 1em;
	padding: 0;
	margin: 0;
	list-style: none;
}
.nav-item {
	display: inline-block;
}
.nav-item a[href]:not(:hover) {
	text-decoration: none;
}
.nav a[href][aria-current="page"] {
	text-decoration: underline;
}

/* Posts list */
.postlist {
	counter-reset: start-from var(--postlist-index);
	list-style: none;
	padding: 0;
	padding-left: 1.5rem;
}
.postlist-item {
	display: flex;
	flex-wrap: wrap;
	align-items: baseline;
	counter-increment: start-from -1;
	margin-bottom: 1em;
}
.postlist-item:before {
	display: inline-block;
	pointer-events: none;
	content: "" counter(start-from, decimal-leading-zero) ". ";
	line-height: 100%;
	text-align: right;
	margin-left: -1.5rem;
}
.postlist-date,
.postlist-item:before {
	font-size: 0.8125em; /* 13px /16 */
	color: var(--color-gray-90);
}
.postlist-date {
	word-spacing: -0.5px;
}
.postlist-link {
	font-size: 1.1875em; /* 19px /16 */
	font-weight: 700;
	flex-basis: calc(100% - 1.5rem);
	padding-left: .25em;
	padding-right: .5em;
	text-underline-position: from-font;
	text-underline-offset: 0;
	text-decoration-thickness: 1px;
}
.postlist-item-active .postlist-link {
	font-weight: bold;
}

/* Tags */
.post-tag {
	display: inline-flex;
	align-items: center;
	justify-content: center;
	text-transform: capitalize;
	font-style: italic;
}
.postlist-item > .post-tag {
	align-self: center;
}

/* Tags list */
.post-metadata {
	display: inline-flex;
	flex-wrap: wrap;
	gap: .5em;
	list-style: none;
	padding: 0;
	margin: 0;
}
.post-metadata time {
	margin-right: 1em;
}
/**
 * okaidia theme for JavaScript, CSS and HTML
 * Loosely based on Monokai textmate theme by http://www.monokai.nl/
 * @author ocodia
 */

code[class*="language-"],
pre[class*="language-"] {
	color: #f8f8f2;
	background: none;
	text-shadow: 0 1px rgba(0, 0, 0, 0.3);
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	font-size: 1em;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
	border-radius: 0.3em;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #272822;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #8292a2;
}

.token.punctuation {
	color: #f8f8f2;
}

.token.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.constant,
.token.symbol,
.token.deleted {
	color: #f92672;
}

.token.boolean,
.token.number {
	color: #ae81ff;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #a6e22e;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string,
.token.variable {
	color: #f8f8f2;
}

.token.atrule,
.token.attr-value,
.token.function,
.token.class-name {
	color: #e6db74;
}

.token.keyword {
	color: #66d9ef;
}

.token.regex,
.token.important {
	color: #fd971f;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
/*
 * New diff- syntax
 */

pre[class*="language-diff-"] {
	--eleventy-code-padding: 1.25em;
	padding-left: var(--eleventy-code-padding);
	padding-right: var(--eleventy-code-padding);
}
.token.deleted {
	background-color: hsl(0, 51%, 37%);
	color: inherit;
}
.token.inserted {
	background-color: hsl(126, 31%, 39%);
	color: inherit;
}

/* Make the + and - characters unselectable for copy/paste */
.token.prefix.unchanged,
.token.prefix.inserted,
.token.prefix.deleted {
	-webkit-user-select: none;
	user-select: none;
	display: inline-flex;
	align-items: center;
	justify-content: center;
	padding-top: 2px;
	padding-bottom: 2px;
}
.token.prefix.inserted,
.token.prefix.deleted {
	width: var(--eleventy-code-padding);
	background-color: rgba(0,0,0,.2);
}

/* Optional: full-width background color */
.token.inserted:not(.prefix),
.token.deleted:not(.prefix) {
	display: block;
	margin-left: calc(-1 * var(--eleventy-code-padding));
	margin-right: calc(-1 * var(--eleventy-code-padding));
	text-decoration: none; /* override del, ins, mark defaults */
	color: inherit; /* override del, ins, mark defaults */
}</style>
		
	</head>
	<body>
		<a href="#main" id="skip-link" class="visually-hidden">Skip to main content</a>

		<header>
			<a href="/" class="home-link">The Sumit&#39;s AI Blog</a>
			<nav>
				<h2 class="visually-hidden" id="top-level-navigation-menu">Top level navigation menu</h2>
				<ul class="nav">
					<li class="nav-item"><a href="/">Home</a></li>
					<li class="nav-item"><a href="/blog/">Archive</a></li>
					<li class="nav-item"><a href="/about/">About</a></li>
					<li class="nav-item"><a href="/feed/feed.xml">Feed</a></li>
				</ul>
			</nav>
		</header>

		<main id="main">
			<heading-anchors>
				


<h1 id="fine-tune-a-pre-trained-model-for-custom-domain">Fine-tune a Pre-trained model for Custom Domain</h1>

<ul class="post-metadata">
	<li><time datetime="2026-01-21">21 January 2026</time></li>
	<li><a href="/tags/nvidia/" class="post-tag">nvidia</a>, </li>
	<li><a href="/tags/transformers/" class="post-tag">transformers</a>, </li>
	<li><a href="/tags/model/" class="post-tag">model</a>, </li>
	<li><a href="/tags/token/" class="post-tag">token</a></li>
</ul>

<h1 id="fine-tune-a-pre-trained-model-for-custom-domain-2">Fine-tune a pre-trained model for custom domain</h1>
<p>An NER model is typically comprised of a pre-trained BERT model followed by a token classification layer.</p>
<p>For training, the config file consists of sections such as:</p>
<ul>
<li><strong>model</strong>: language model, datasets, token classifier, optimizer and schedulers.</li>
<li><strong>trainer</strong>: any argument that is to be passed to PyTorch Lightning.</li>
</ul>
<p>For beginners, NeMo provides a starter configuration file.</p>
<h2 id="configuration-file">Configuration File</h2>
<pre class="language-python" tabindex="0"><code class="language-python"><span class="token comment"># define config path</span>
MODEL_CONFIG <span class="token operator">=</span> <span class="token string">"token_classification_config.yaml"</span>
WORK_DIR <span class="token operator">=</span> <span class="token string">"WORK_DIR"</span>
os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>WORK_DIR<span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># download the model's configuration file </span>
BRANCH <span class="token operator">=</span> <span class="token string">'main'</span>
config_dir <span class="token operator">=</span> WORK_DIR <span class="token operator">+</span> <span class="token string">'/configs/'</span>
os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>config_dir<span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>config_dir <span class="token operator">+</span> MODEL_CONFIG<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Downloading config file...'</span><span class="token punctuation">)</span>
    wget<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'https://raw.githubusercontent.com/NVIDIA/NeMo/</span><span class="token interpolation"><span class="token punctuation">{</span>BRANCH<span class="token punctuation">}</span></span><span class="token string">/examples/nlp/token_classification/conf/'</span></span> <span class="token operator">+</span> MODEL_CONFIG<span class="token punctuation">,</span> config_dir<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'config file already exists'</span><span class="token punctuation">)</span></code></pre>
<p>Here, the config file for NER, <code>token_classification_config.yaml</code>, specifies model, training and experiment management details such as file locations, pretrained models and hyperparameters.</p>
<pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">from</span> omegaconf <span class="token keyword">import</span> OmegaConf

CONFIG_DIR <span class="token operator">=</span> <span class="token string">"/dli/task/WORK_DIR/configs"</span>
CONFIG_FILE <span class="token operator">=</span> <span class="token string">"token_classification_config.yaml"</span>

config<span class="token operator">=</span>OmegaConf<span class="token punctuation">.</span>load<span class="token punctuation">(</span>CONFIG_DIR <span class="token operator">+</span> <span class="token string">"/"</span> <span class="token operator">+</span> CONFIG_FILE<span class="token punctuation">)</span>

<span class="token comment"># print the entire configuration file</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>OmegaConf<span class="token punctuation">.</span>to_yaml<span class="token punctuation">(</span>config<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<h4 id="output">output</h4>
<pre class="language-bash" tabindex="0"><code class="language-bash">pretrained_model: null
trainer:
  devices: <span class="token number">1</span>
  num_nodes: <span class="token number">1</span>
  max_epochs: <span class="token number">5</span>
  max_steps: <span class="token parameter variable">-1</span>
  accumulate_grad_batches: <span class="token number">1</span>
  gradient_clip_val: <span class="token number">0.0</span>
  precision: <span class="token number">16</span>
  accelerator: gpu
  enable_checkpointing: <span class="token boolean">false</span>
  logger: <span class="token boolean">false</span>
  log_every_n_steps: <span class="token number">1</span>
  val_check_interval: <span class="token number">1.0</span>
exp_manager:
  exp_dir: null
  name: token_classification_model
  create_tensorboard_logger: <span class="token boolean">true</span>
  create_checkpoint_callback: <span class="token boolean">true</span>
model:
  label_ids: null
  class_labels:
    class_labels_file: label_ids.csv
  dataset:
    data_dir: ???
    class_balancing: null
    max_seq_length: <span class="token number">128</span>
    pad_label: O
    ignore_extra_tokens: <span class="token boolean">false</span>
    ignore_start_end: <span class="token boolean">false</span>
    use_cache: <span class="token boolean">false</span>
    num_workers: <span class="token number">2</span>
    pin_memory: <span class="token boolean">false</span>
    drop_last: <span class="token boolean">false</span>
  train_ds:
    text_file: text_train.txt
    labels_file: labels_train.txt
    shuffle: <span class="token boolean">true</span>
    num_samples: <span class="token parameter variable">-1</span>
    batch_size: <span class="token number">64</span>
  validation_ds:
    text_file: text_dev.txt
    labels_file: labels_dev.txt
    shuffle: <span class="token boolean">false</span>
    num_samples: <span class="token parameter variable">-1</span>
    batch_size: <span class="token number">64</span>
  test_ds:
    text_file: text_dev.txt
    labels_file: labels_dev.txt
    shuffle: <span class="token boolean">false</span>
    num_samples: <span class="token parameter variable">-1</span>
    batch_size: <span class="token number">64</span>
  tokenizer:
    tokenizer_name: <span class="token variable">${model.language_model.pretrained_model_name}</span>
    vocab_file: null
    tokenizer_model: null
    special_tokens: null
  language_model:
    pretrained_model_name: bert-base-uncased
    lm_checkpoint: null
    config_file: null
    config: null
  head:
    num_fc_layers: <span class="token number">2</span>
    fc_dropout: <span class="token number">0.5</span>
    activation: relu
    use_transformer_init: <span class="token boolean">true</span>
  optim:
    name: adam
    lr: <span class="token number">5</span>.0e-05
    weight_decay: <span class="token number">0.0</span>
    sched:
      name: WarmupAnnealing
      warmup_steps: null
      warmup_ratio: <span class="token number">0.1</span>
      last_epoch: <span class="token parameter variable">-1</span>
      monitor: val_loss
      reduce_on_plateau: <span class="token boolean">false</span>
hydra:
  run:
    dir: <span class="token builtin class-name">.</span>
  job_logging:
    root:
      handlers: null</code></pre>
<pre class="language-python" tabindex="0"><code class="language-python"><span class="token comment"># in this exercise, train and dev datasets are located in the same folder under the default names, </span>
<span class="token comment"># so it is enough to add the path of the data directory to the config</span>
config<span class="token punctuation">.</span>model<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>data_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>DATA_DIR<span class="token punctuation">,</span> <span class="token string">'NER'</span><span class="token punctuation">)</span>

<span class="token comment"># print the model section</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>OmegaConf<span class="token punctuation">.</span>to_yaml<span class="token punctuation">(</span>config<span class="token punctuation">.</span>model<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<h4 id="output-2">output</h4>
<pre class="language-bash" tabindex="0"><code class="language-bash">label_ids: null
class_labels:
  class_labels_file: label_ids.csv
dataset:
  data_dir: data/NCBI/NER
  class_balancing: null
  max_seq_length: <span class="token number">128</span>
  pad_label: O
  ignore_extra_tokens: <span class="token boolean">false</span>
  ignore_start_end: <span class="token boolean">false</span>
  use_cache: <span class="token boolean">false</span>
  num_workers: <span class="token number">2</span>
  pin_memory: <span class="token boolean">false</span>
  drop_last: <span class="token boolean">false</span>
train_ds:
  text_file: text_train.txt
  labels_file: labels_train.txt
  shuffle: <span class="token boolean">true</span>
  num_samples: <span class="token parameter variable">-1</span>
  batch_size: <span class="token number">64</span>
validation_ds:
  text_file: text_dev.txt
  labels_file: labels_dev.txt
  shuffle: <span class="token boolean">false</span>
  num_samples: <span class="token parameter variable">-1</span>
  batch_size: <span class="token number">64</span>
test_ds:
  text_file: text_dev.txt
  labels_file: labels_dev.txt
  shuffle: <span class="token boolean">false</span>
  num_samples: <span class="token parameter variable">-1</span>
  batch_size: <span class="token number">64</span>
tokenizer:
  tokenizer_name: <span class="token variable">${model.language_model.pretrained_model_name}</span>
  vocab_file: null
  tokenizer_model: null
  special_tokens: null
language_model:
  pretrained_model_name: bert-base-uncased
  lm_checkpoint: null
  config_file: null
  config: null
head:
  num_fc_layers: <span class="token number">2</span>
  fc_dropout: <span class="token number">0.5</span>
  activation: relu
  use_transformer_init: <span class="token boolean">true</span>
optim:
  name: adam
  lr: <span class="token number">5</span>.0e-05
  weight_decay: <span class="token number">0.0</span>
  sched:
    name: WarmupAnnealing
    warmup_steps: null
    warmup_ratio: <span class="token number">0.1</span>
    last_epoch: <span class="token parameter variable">-1</span>
    monitor: val_loss
    reduce_on_plateau: <span class="token boolean">false</span></code></pre>
<h2 id="download-domain-specific-pre-trained-models">Download domain-specific pre-trained models</h2>

<ul class="links-nextprev"><li class="links-nextprev-prev">← Previous<br> <a href="/blog/domain-token-classification-model/">Domain specific Token Classification Model</a></li><li class="links-nextprev-next">Next →<br><a href="/blog/transformers-natural-language-processing/">Transformers-based Natural Language Processing</a></li>
</ul>

			</heading-anchors>
		</main>

		<footer>
		</footer>

		<!-- This page `/blog/fine-tune-pre-trained-model-custom-domain/` was built on 2026-01-21T20:12:30.422Z -->
		<script type="module" src="/dist/cC8wS6ZjFU.js"></script>
	</body>
</html>
