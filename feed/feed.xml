<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="pretty-atom-feed.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Blog Title</title>
  <subtitle>This is a longer description about your blog.</subtitle>
  <link href="https://example.com/feed/feed.xml" rel="self" />
  <link href="https://example.com/" />
  <updated>2026-01-21T00:00:00Z</updated>
  <id>https://example.com/</id>
  <author>
    <name>Your Name</name>
  </author>
  <entry>
    <title>Transformers-based Natural Language Processing</title>
    <link href="https://example.com/blog/transformers-natural-language-processing/" />
    <updated>2026-01-21T00:00:00Z</updated>
    <id>https://example.com/blog/transformers-natural-language-processing/</id>
    <content type="html">&lt;h2 id=&quot;download-and-preprocess-data&quot;&gt;Download and Preprocess Data&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import os
import wget

# set data path
DATA_DIR=&amp;quot;data/GMB&amp;quot;

# check that data folder should contain 4 files
!ls -l $DATA_DIR
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;output&quot;&gt;output&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;total 11140
-rw-r--r-- 1 root root      77 Jan 21 16:38 label_ids.csv
-rw-r--r-- 1 root root  407442 Jan 21 16:38 labels_dev.txt
-rw-r--r-- 1 root root 3169783 Jan 21 16:38 labels_train.txt
-rw-r--r-- 1 root root  891020 Jan 21 16:38 text_dev.txt
-rw-r--r-- 1 root root 6928251 Jan 21 16:38 text_train.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# preview data 
print(&#39;Text:&#39;)
!head -n 5 {DATA_DIR}/text_train.txt

print(&#39;Labels:&#39;)
!head -n 5 {DATA_DIR}/labels_train.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;output-2&quot;&gt;output&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Text:
New Zealand &#39;s cricket team has scored a morale-boosting win over Bangladesh in the first of three one-day internationals in New Zealand .
Despite Bangladesh &#39;s highest total ever in a limited-overs match , the Kiwis were able to win the match by six wickets in Auckland .
Opening batsman Jamie How led all scorers with 88 runs as New Zealand reached 203-4 in 42.1 overs .
The score was in response to Bangladesh &#39;s total of 201 all out in 46.3 overs .
Mohammad Ashraful led the visitors with 70 runs , including 10 fours and one six on the short boundaries of the Eden Park ground .
Labels:
B-LOC I-LOC O O O O O O O O O B-LOC O O B-TIME I-TIME I-TIME I-TIME O O B-LOC I-LOC O
O B-LOC O O O O O O O O O O B-GPE O O O O O O O O O O B-LOC O
O O B-PER I-PER O O O O O O O B-LOC I-LOC O O O O O O
O O O O O O B-LOC O O O O O O O O O O
B-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;download-pre-trained-model&quot;&gt;Download Pre-trained model&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# import dependencies
from nemo.collections.nlp.models import TokenClassificationModel

# list available pre-trained models
for model in TokenClassificationModel.list_available_models():
    print(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;output-3&quot;&gt;output&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;NOTE! Installing ujson may make loading annotations faster.
PretrainedModelInfo(
	pretrained_model_name=ner_en_bert,
	description=The model was trained on GMB (Groningen Meaning Bank) corpus for entity recognition and achieves 74.61 F1 Macro score.,
	location=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/ner_en_bert/versions/1.10/files/ner_en_bert.nemo
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# download and load the pre-trained BERT-based model
pretrained_ner_model=TokenClassificationModel.from_pretrained(&amp;quot;ner_en_bert&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;output-4&quot;&gt;output&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# download and load the pre-trained BERT-based model
pretrained_ner_model=TokenClassificationModel.from_pretrained(&amp;quot;ner_en_bert&amp;quot;)
# download and load the pre-trained BERT-based model
pretrained_ner_model=TokenClassificationModel.from_pretrained(&amp;quot;ner_en_bert&amp;quot;)
[NeMo I 2026-01-21 17:08:48 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/ner_en_bert/versions/1.10/files/ner_en_bert.nemo to /root/.cache/torch/NeMo/NeMo_1.20.0/ner_en_bert/8186f86c83b11d70b43b9ead695e7eda/ner_en_bert.nemo
[NeMo I 2026-01-21 17:08:49 common:913] Instantiating model from pre-trained checkpoint
[NeMo I 2026-01-21 17:08:52 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmp9g1j_hsl/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False
Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00&amp;lt;?, ?B/s]
Downloading config.json:   0%|          | 0.00/570 [00:00&amp;lt;?, ?B/s]
Downloading vocab.txt:   0%|          | 0.00/232k [00:00&amp;lt;?, ?B/s]
Using eos_token, but it is not set yet.
Using bos_token, but it is not set yet.
[NeMo W 2026-01-21 17:08:53 modelPT:244] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.
[NeMo W 2026-01-21 17:08:53 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    text_file: text_train.txt
    labels_file: labels_train.txt
    shuffle: true
    num_samples: -1
    batch_size: 64
    
[NeMo W 2026-01-21 17:08:53 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    text_file: text_dev.txt
    labels_file: labels_dev.txt
    shuffle: false
    num_samples: -1
    batch_size: 64
    
[NeMo W 2026-01-21 17:08:53 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).
    Test config : 
    text_file: text_dev.txt
    labels_file: labels_dev.txt
    shuffle: false
    num_samples: -1
    batch_size: 64
    
Downloading model.safetensors:   0%|          | 0.00/440M [00:00&amp;lt;?, ?B/s]
[NeMo W 2026-01-21 17:08:56 modelPT:244] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.
[NeMo I 2026-01-21 17:08:57 save_restore_connector:249] Model TokenClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.20.0/ner_en_bert/8186f86c83b11d70b43b9ead695e7eda/ner_en_bert.nemo.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;make-predictions&quot;&gt;Make Predictions&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# define the list of queries for inference
queries=[
    &#39;we bought four shirts from the nvidia gear store in santa clara.&#39;,
    &#39;Nvidia is a company.&#39;,
]

# make sample predictions
results=pretrained_ner_model.add_predictions(queries)

# show predictions
for query, result in zip(queries, results):
    print(f&#39;Query : {query}&#39;)
    print(f&#39;Result: {result.strip()}&#92;n&#39;)
    print()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;output-5&quot;&gt;output&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# define the list of queries for inference
queries=[
    &#39;we bought four shirts from the nvidia gear store in santa clara.&#39;,
    &#39;Nvidia is a company.&#39;,
]

# make sample predictions
results=pretrained_ner_model.add_predictions(queries)

# show predictions
for query, result in zip(queries, results):
    print(f&#39;Query : {query}&#39;)
    print(f&#39;Result: {result.strip()}&#92;n&#39;)
    print()
# define the list of queries for inference
queries=[
    &#39;we bought four shirts from the nvidia gear store in santa clara.&#39;,
    &#39;Nvidia is a company.&#39;,
]
​
# make sample predictions
results=pretrained_ner_model.add_predictions(queries)
​
# show predictions
for query, result in zip(queries, results):
    print(f&#39;Query : {query}&#39;)
    print(f&#39;Result: {result.strip()}&#92;n&#39;)
    print()
[NeMo I 2026-01-21 17:10:06 token_classification_dataset:123] Setting Max Seq length to: 17
[NeMo I 2026-01-21 17:10:06 data_preprocessing:404] Some stats of the lengths of the sequences:
[NeMo I 2026-01-21 17:10:06 data_preprocessing:406] Min: 9 |                  Max: 17 |                  Mean: 13.0 |                  Median: 13.0
[NeMo I 2026-01-21 17:10:06 data_preprocessing:412] 75 percentile: 15.00
[NeMo I 2026-01-21 17:10:06 data_preprocessing:413] 99 percentile: 16.92
[NeMo W 2026-01-21 17:10:06 token_classification_dataset:152] 0 are longer than 17
[NeMo I 2026-01-21 17:10:06 token_classification_dataset:155] *** Example ***
[NeMo I 2026-01-21 17:10:06 token_classification_dataset:156] i: 0
[NeMo I 2026-01-21 17:10:06 token_classification_dataset:157] subtokens: [CLS] we bought four shirts from the n ##vid ##ia gear store in santa clara . [SEP]
[NeMo I 2026-01-21 17:10:06 token_classification_dataset:158] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[NeMo I 2026-01-21 17:10:06 token_classification_dataset:159] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[NeMo I 2026-01-21 17:10:06 token_classification_dataset:160] subtokens_mask: 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0
Query : we bought four shirts from the nvidia gear store in santa clara.
Result: we bought four shirts from the nvidia[B-ORG] gear store in santa[B-LOC] clara[I-LOC].


Query : Nvidia is a company.
Result: Nvidia[B-ORG] is a company.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;evaluate-predictions&quot;&gt;Evaluate Predictions&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# create a subset of our dev data
!head -n 100 $DATA_DIR/text_dev.txt &amp;gt; $DATA_DIR/sample_text_dev.txt
!head -n 100 $DATA_DIR/labels_dev.txt &amp;gt; $DATA_DIR/sample_labels_dev.txt

WORK_DIR = &amp;quot;WORK_DIR&amp;quot;

# evaluate model performance on sample
pretrained_ner_model.evaluate_from_file(
    text_file=os.path.join(DATA_DIR, &#39;sample_text_dev.txt&#39;),
    labels_file=os.path.join(DATA_DIR, &#39;sample_labels_dev.txt&#39;),
    output_dir=WORK_DIR,
    add_confusion_matrix=True,
    normalize_confusion_matrix=True,
    batch_size=1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output-6&quot;&gt;output&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[NeMo I 2026-01-21 17:12:02 token_classification_dataset:123] Setting Max Seq length to: 70
[NeMo I 2026-01-21 17:12:02 data_preprocessing:404] Some stats of the lengths of the sequences:
[NeMo I 2026-01-21 17:12:02 data_preprocessing:406] Min: 11 |                  Max: 70 |                  Mean: 26.9 |                  Median: 26.0
[NeMo I 2026-01-21 17:12:02 data_preprocessing:412] 75 percentile: 33.00
[NeMo I 2026-01-21 17:12:02 data_preprocessing:413] 99 percentile: 65.05
[NeMo W 2026-01-21 17:12:02 token_classification_dataset:152] 0 are longer than 70
[NeMo I 2026-01-21 17:12:02 token_classification_dataset:155] *** Example ***
[NeMo I 2026-01-21 17:12:02 token_classification_dataset:156] i: 0
[NeMo I 2026-01-21 17:12:02 token_classification_dataset:157] subtokens: [CLS] hamas refuses to recognize israel , and has vowed to undermine palestinian leader mahmoud abbas &#39; s efforts to make peace with the jewish state . [SEP]
[NeMo I 2026-01-21 17:12:02 token_classification_dataset:158] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[NeMo I 2026-01-21 17:12:02 token_classification_dataset:159] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[NeMo I 2026-01-21 17:12:02 token_classification_dataset:160] subtokens_mask: 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[NeMo I 2026-01-21 17:12:03 token_classification_model:464] Labels save to /dli/task/WORK_DIR/infer_sample_text_dev.txt
[NeMo I 2026-01-21 17:12:03 token_classification_model:470] Predictions saved to /dli/task/WORK_DIR/infer_sample_text_dev.txt
[NeMo I 2026-01-21 17:12:03 utils_funcs:109] Confusion matrix saved to /dli/task/WORK_DIR/Normalized_Confusion_matrix_20260121-171203
[NeMo I 2026-01-21 17:12:03 token_classification_model:481]                        precision    recall  f1-score   support
    
          O (label id: 0)     0.9878    0.9895    0.9887      1805
      B-GPE (label id: 1)     0.9429    1.0000    0.9706        33
      B-LOC (label id: 2)     0.9103    0.9103    0.9103        78
     B-MISC (label id: 3)     0.6667    1.0000    0.8000         2
      B-ORG (label id: 4)     0.8431    0.7544    0.7963        57
      B-PER (label id: 5)     0.8095    0.8644    0.8361        59
     B-TIME (label id: 6)     0.8936    0.9130    0.9032        46
      I-GPE (label id: 7)     1.0000    1.0000    1.0000         4
      I-LOC (label id: 8)     0.8000    0.8889    0.8421         9
     I-ORG (label id: 10)     0.8421    0.6809    0.7529        47
     I-PER (label id: 11)     0.8305    0.8750    0.8522        56
    I-TIME (label id: 12)     0.8462    0.8462    0.8462        13
    
                 accuracy                         0.9651      2209
                macro avg     0.8644    0.8935    0.8749      2209
             weighted avg     0.9650    0.9651    0.9647      2209
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;fine-tune-a-pre-trained-model&quot;&gt;Fine-tune a Pre-trained model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Without specifying any config file, Nemo will use the default configurations for the model and trainer.&lt;/li&gt;
&lt;li&gt;When fine-tuning a pre-trained NER model, we need to setup training and evaluation data before training.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;import pytorch_lightning as pl

# setup the data dir to get class weights statistics
pretrained_ner_model.update_data_dir(DATA_DIR)

# setup train and validation Pytorch DataLoaders
pretrained_ner_model.setup_training_data()
pretrained_ner_model.setup_validation_data()
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output-7&quot;&gt;output&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[NeMo I 2026-01-21 17:18:54 token_classification_model:84] Setting model.dataset.data_dir to data/GMB.
[NeMo I 2026-01-21 17:18:54 token_classification_utils:118] Processing data/GMB/labels_train.txt
[NeMo I 2026-01-21 17:18:54 token_classification_utils:138] Using provided labels mapping {&#39;O&#39;: 0, &#39;B-GPE&#39;: 1, &#39;B-LOC&#39;: 2, &#39;B-MISC&#39;: 3, &#39;B-ORG&#39;: 4, &#39;B-PER&#39;: 5, &#39;B-TIME&#39;: 6, &#39;I-GPE&#39;: 7, &#39;I-LOC&#39;: 8, &#39;I-MISC&#39;: 9, &#39;I-ORG&#39;: 10, &#39;I-PER&#39;: 11, &#39;I-TIME&#39;: 12}
[NeMo I 2026-01-21 17:18:54 token_classification_utils:154] Labels mapping {&#39;O&#39;: 0, &#39;B-GPE&#39;: 1, &#39;B-LOC&#39;: 2, &#39;B-MISC&#39;: 3, &#39;B-ORG&#39;: 4, &#39;B-PER&#39;: 5, &#39;B-TIME&#39;: 6, &#39;I-GPE&#39;: 7, &#39;I-LOC&#39;: 8, &#39;I-MISC&#39;: 9, &#39;I-ORG&#39;: 10, &#39;I-PER&#39;: 11, &#39;I-TIME&#39;: 12} saved to : /dli/task/data/GMB/label_ids.csv
[NeMo I 2026-01-21 17:19:09 token_classification_utils:163] Three most popular labels in data/GMB/labels_train.txt:
[NeMo I 2026-01-21 17:19:09 data_preprocessing:194] label: 0, 1014899 out of 1199472 (84.61%).
[NeMo I 2026-01-21 17:19:09 data_preprocessing:194] label: 2, 43529 out of 1199472 (3.63%).
[NeMo I 2026-01-21 17:19:09 data_preprocessing:194] label: 6, 23321 out of 1199472 (1.94%).
[NeMo I 2026-01-21 17:19:09 token_classification_utils:165] Total labels: 1199472. Label frequencies - {0: 1014899, 2: 43529, 6: 23321, 4: 23215, 11: 19583, 10: 19515, 5: 19407, 1: 18074, 8: 8482, 12: 7555, 3: 1002, 9: 669, 7: 221}
[NeMo I 2026-01-21 17:19:09 token_classification_utils:171] Class weights restored from data/GMB/labels_train_weights.p
[NeMo W 2026-01-21 17:19:09 modelPT:244] You tried to register an artifact under config key=class_labels.class_labels_file but an artifact for it has already been registered.
[NeMo I 2026-01-21 17:19:12 token_classification_dataset:287] features restored from data/GMB/cached__text_train.txt__labels_train.txt__BertTokenizer_128_30522_-1
[NeMo I 2026-01-21 17:19:12 token_classification_utils:118] Processing data/GMB/labels_dev.txt
[NeMo I 2026-01-21 17:19:12 token_classification_utils:138] Using provided labels mapping {&#39;O&#39;: 0, &#39;B-GPE&#39;: 1, &#39;B-LOC&#39;: 2, &#39;B-MISC&#39;: 3, &#39;B-ORG&#39;: 4, &#39;B-PER&#39;: 5, &#39;B-TIME&#39;: 6, &#39;I-GPE&#39;: 7, &#39;I-LOC&#39;: 8, &#39;I-MISC&#39;: 9, &#39;I-ORG&#39;: 10, &#39;I-PER&#39;: 11, &#39;I-TIME&#39;: 12}
[NeMo I 2026-01-21 17:19:12 token_classification_utils:160] data/GMB/labels_dev_label_stats.tsv found, skipping stats calculation.
[NeMo I 2026-01-21 17:19:12 token_classification_dataset:287] features restored from data/GMB/cached__text_dev.txt__labels_dev.txt__BertTokenizer_128_30522_-1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# set up loss
pretrained_ner_model.setup_loss()
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output-8&quot;&gt;output&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CrossEntropyLoss()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# create a PyTorch Lightning trainer and call `fit` again
fast_dev_run=True
trainer=pl.Trainer(devices=1, accelerator=&#39;gpu&#39;, fast_dev_run=fast_dev_run)
trainer.fit(pretrained_ner_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output-9&quot;&gt;output&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[NeMo I 2026-01-21 17:18:51 modelPT:721] Optimizer config = Adam (
    Parameter Group 0
        amsgrad: False
        betas: (0.9, 0.999)
        capturable: False
        differentiable: False
        eps: 1e-08
        foreach: None
        fused: None
        lr: 5e-05
        maximize: False
        weight_decay: 0.0
    )
[NeMo I 2026-01-21 17:18:51 lr_scheduler:910] Scheduler &amp;quot;&amp;lt;nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fba462776d0&amp;gt;&amp;quot; 
    will be used during training (effective maximum steps = 1) - 
    Parameters : 
    (warmup_steps: null
    warmup_ratio: 0.1
    last_epoch: -1
    max_steps: 1
    )

  | Name                  | Type                 | Params
---------------------------------------------------------------
0 | bert_model            | BertEncoder          | 109 M 
1 | classifier            | TokenClassifier      | 600 K 
2 | loss                  | CrossEntropyLoss     | 0     
3 | classification_report | ClassificationReport | 0     
---------------------------------------------------------------
110 M     Trainable params
0         Non-trainable params
110 M     Total params
440.331   Total estimated model params size (MB)
[NeMo W 2026-01-21 17:18:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      rank_zero_warn(
    
[NeMo W 2026-01-21 17:18:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
      rank_zero_warn(
    
[NeMo W 2026-01-21 17:18:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      rank_zero_warn(
    
Training: 0it [00:00, ?it/s]
Validation: 0it [00:00, ?it/s]
[NeMo I 2026-01-21 17:18:51 token_classification_model:159] 
    label                                                precision    recall       f1           support   
    O (label_id: 0)                                         98.89      99.48      99.18       1161
    B-GPE (label_id: 1)                                     90.91     100.00      95.24         20
    B-LOC (label_id: 2)                                     82.69      97.73      89.58         44
    B-MISC (label_id: 3)                                   100.00     100.00     100.00          2
    B-ORG (label_id: 4)                                     85.29      65.91      74.36         44
    B-PER (label_id: 5)                                     85.11      88.89      86.96         45
    B-TIME (label_id: 6)                                    95.83     100.00      97.87         23
    I-GPE (label_id: 7)                                    100.00     100.00     100.00          4
    I-LOC (label_id: 8)                                     66.67      80.00      72.73          5
    I-MISC (label_id: 9)                                     0.00       0.00       0.00          0
    I-ORG (label_id: 10)                                    86.36      63.33      73.08         30
    I-PER (label_id: 11)                                    92.11      85.37      88.61         41
    I-TIME (label_id: 12)                                  100.00     100.00     100.00          7
    -------------------
    micro avg                                               96.84      96.84      96.84       1426
    macro avg                                               90.32      90.06      89.80       1426
    weighted avg                                            96.81      96.84      96.72       1426
    
`Trainer.fit` stopped: `max_steps=1` reached.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# evaluate model performance on sample
pretrained_ner_model.evaluate_from_file(
    text_file=os.path.join(DATA_DIR, &#39;sample_text_dev.txt&#39;),
    labels_file=os.path.join(DATA_DIR, &#39;sample_labels_dev.txt&#39;),
    output_dir=WORK_DIR,
    add_confusion_matrix=True,
    normalize_confusion_matrix=True,
    batch_size=1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output-10&quot;&gt;output&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[NeMo I 2026-01-21 17:18:52 token_classification_dataset:123] Setting Max Seq length to: 70
[NeMo I 2026-01-21 17:18:52 data_preprocessing:404] Some stats of the lengths of the sequences:
[NeMo I 2026-01-21 17:18:52 data_preprocessing:406] Min: 11 |                  Max: 70 |                  Mean: 26.9 |                  Median: 26.0
[NeMo I 2026-01-21 17:18:52 data_preprocessing:412] 75 percentile: 33.00
[NeMo I 2026-01-21 17:18:52 data_preprocessing:413] 99 percentile: 65.05
[NeMo W 2026-01-21 17:18:52 token_classification_dataset:152] 0 are longer than 70
[NeMo I 2026-01-21 17:18:52 token_classification_dataset:155] *** Example ***
[NeMo I 2026-01-21 17:18:52 token_classification_dataset:156] i: 0
[NeMo I 2026-01-21 17:18:52 token_classification_dataset:157] subtokens: [CLS] hamas refuses to recognize israel , and has vowed to undermine palestinian leader mahmoud abbas &#39; s efforts to make peace with the jewish state . [SEP]
[NeMo I 2026-01-21 17:18:52 token_classification_dataset:158] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[NeMo I 2026-01-21 17:18:52 token_classification_dataset:159] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[NeMo I 2026-01-21 17:18:52 token_classification_dataset:160] subtokens_mask: 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[NeMo I 2026-01-21 17:18:53 token_classification_model:464] Labels save to /dli/task/WORK_DIR/infer_sample_text_dev.txt
[NeMo I 2026-01-21 17:18:53 token_classification_model:470] Predictions saved to /dli/task/WORK_DIR/infer_sample_text_dev.txt
[NeMo I 2026-01-21 17:18:53 utils_funcs:109] Confusion matrix saved to /dli/task/WORK_DIR/Normalized_Confusion_matrix_20260121-171853
[NeMo I 2026-01-21 17:18:53 token_classification_model:481]                        precision    recall  f1-score   support
    
          O (label id: 0)     0.9878    0.9895    0.9887      1805
      B-GPE (label id: 1)     0.9429    1.0000    0.9706        33
      B-LOC (label id: 2)     0.8556    0.9872    0.9167        78
     B-MISC (label id: 3)     1.0000    1.0000    1.0000         2
      B-ORG (label id: 4)     0.8636    0.6667    0.7525        57
      B-PER (label id: 5)     0.7794    0.8983    0.8346        59
     B-TIME (label id: 6)     0.9149    0.9348    0.9247        46
      I-GPE (label id: 7)     1.0000    1.0000    1.0000         4
      I-LOC (label id: 8)     0.7778    0.7778    0.7778         9
     I-ORG (label id: 10)     0.8611    0.6596    0.7470        47
     I-PER (label id: 11)     0.8868    0.8393    0.8624        56
    I-TIME (label id: 12)     0.8462    0.8462    0.8462        13
    
                 accuracy                         0.9651      2209
                macro avg     0.8930    0.8833    0.8851      2209
             weighted avg     0.9653    0.9651    0.9643      2209
    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# restart the kernel
import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)
&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
  <entry>
    <title>Fine-tune a Pre-trained model for Custom Domain</title>
    <link href="https://example.com/blog/fine-tune-pre-trained-model-custom-domain/" />
    <updated>2026-01-21T00:00:00Z</updated>
    <id>https://example.com/blog/fine-tune-pre-trained-model-custom-domain/</id>
    <content type="html">&lt;h1 id=&quot;fine-tune-a-pre-trained-model-for-custom-domain&quot;&gt;Fine-tune a pre-trained model for custom domain&lt;/h1&gt;
&lt;p&gt;An NER model is typically comprised of a pre-trained BERT model followed by a token classification layer.&lt;/p&gt;
&lt;p&gt;For training, the config file consists of sections such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;model&lt;/strong&gt;: language model, datasets, token classifier, optimizer and schedulers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;trainer&lt;/strong&gt;: any argument that is to be passed to PyTorch Lightning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For beginners, NeMo provides a starter configuration file.&lt;/p&gt;
&lt;h2 id=&quot;configuration-file&quot;&gt;Configuration File&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;# define config path
MODEL_CONFIG = &amp;quot;token_classification_config.yaml&amp;quot;
WORK_DIR = &amp;quot;WORK_DIR&amp;quot;
os.makedirs(WORK_DIR, exist_ok=True)

# download the model&#39;s configuration file 
BRANCH = &#39;main&#39;
config_dir = WORK_DIR + &#39;/configs/&#39;
os.makedirs(config_dir, exist_ok=True)

if not os.path.exists(config_dir + MODEL_CONFIG):
    print(&#39;Downloading config file...&#39;)
    wget.download(f&#39;https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/token_classification/conf/&#39; + MODEL_CONFIG, config_dir)
else:
    print (&#39;config file already exists&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the config file for NER, &lt;code&gt;token_classification_config.yaml&lt;/code&gt;, specifies model, training and experiment management details such as file locations, pretrained models and hyperparameters.&lt;/p&gt;
&lt;pre class=&quot;language-python&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; omegaconf &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; OmegaConf

CONFIG_DIR &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;/dli/task/WORK_DIR/configs&quot;&lt;/span&gt;
CONFIG_FILE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;token_classification_config.yaml&quot;&lt;/span&gt;

config&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;OmegaConf&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;CONFIG_DIR &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;/&quot;&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; CONFIG_FILE&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# print the entire configuration file&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;OmegaConf&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;to_yaml&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;config&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output&quot;&gt;output&lt;/h4&gt;
&lt;pre class=&quot;language-bash&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;pretrained_model: null
trainer:
  devices: &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
  num_nodes: &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
  max_epochs: &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;
  max_steps: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
  accumulate_grad_batches: &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
  gradient_clip_val: &lt;span class=&quot;token number&quot;&gt;0.0&lt;/span&gt;
  precision: &lt;span class=&quot;token number&quot;&gt;16&lt;/span&gt;
  accelerator: gpu
  enable_checkpointing: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  logger: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  log_every_n_steps: &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
  val_check_interval: &lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt;
exp_manager:
  exp_dir: null
  name: token_classification_model
  create_tensorboard_logger: &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
  create_checkpoint_callback: &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
model:
  label_ids: null
  class_labels:
    class_labels_file: label_ids.csv
  dataset:
    data_dir: ???
    class_balancing: null
    max_seq_length: &lt;span class=&quot;token number&quot;&gt;128&lt;/span&gt;
    pad_label: O
    ignore_extra_tokens: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
    ignore_start_end: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
    use_cache: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
    num_workers: &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;
    pin_memory: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
    drop_last: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  train_ds:
    text_file: text_train.txt
    labels_file: labels_train.txt
    shuffle: &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
    num_samples: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
    batch_size: &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
  validation_ds:
    text_file: text_dev.txt
    labels_file: labels_dev.txt
    shuffle: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
    num_samples: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
    batch_size: &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
  test_ds:
    text_file: text_dev.txt
    labels_file: labels_dev.txt
    shuffle: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
    num_samples: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
    batch_size: &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
  tokenizer:
    tokenizer_name: &lt;span class=&quot;token variable&quot;&gt;${model.language_model.pretrained_model_name}&lt;/span&gt;
    vocab_file: null
    tokenizer_model: null
    special_tokens: null
  language_model:
    pretrained_model_name: bert-base-uncased
    lm_checkpoint: null
    config_file: null
    config: null
  head:
    num_fc_layers: &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;
    fc_dropout: &lt;span class=&quot;token number&quot;&gt;0.5&lt;/span&gt;
    activation: relu
    use_transformer_init: &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
  optim:
    name: adam
    lr: &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;.0e-05
    weight_decay: &lt;span class=&quot;token number&quot;&gt;0.0&lt;/span&gt;
    sched:
      name: WarmupAnnealing
      warmup_steps: null
      warmup_ratio: &lt;span class=&quot;token number&quot;&gt;0.1&lt;/span&gt;
      last_epoch: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
      monitor: val_loss
      reduce_on_plateau: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
hydra:
  run:
    dir: &lt;span class=&quot;token builtin class-name&quot;&gt;.&lt;/span&gt;
  job_logging:
    root:
      handlers: null&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-python&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# in this exercise, train and dev datasets are located in the same folder under the default names, &lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# so it is enough to add the path of the data directory to the config&lt;/span&gt;
config&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;model&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;dataset&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;data_dir &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; os&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;join&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;DATA_DIR&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&#39;NER&#39;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# print the model section&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;OmegaConf&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;to_yaml&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;config&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;model&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output-2&quot;&gt;output&lt;/h4&gt;
&lt;pre class=&quot;language-bash&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;label_ids: null
class_labels:
  class_labels_file: label_ids.csv
dataset:
  data_dir: data/NCBI/NER
  class_balancing: null
  max_seq_length: &lt;span class=&quot;token number&quot;&gt;128&lt;/span&gt;
  pad_label: O
  ignore_extra_tokens: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  ignore_start_end: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  use_cache: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  num_workers: &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;
  pin_memory: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  drop_last: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
train_ds:
  text_file: text_train.txt
  labels_file: labels_train.txt
  shuffle: &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
  num_samples: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
  batch_size: &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
validation_ds:
  text_file: text_dev.txt
  labels_file: labels_dev.txt
  shuffle: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  num_samples: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
  batch_size: &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
test_ds:
  text_file: text_dev.txt
  labels_file: labels_dev.txt
  shuffle: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
  num_samples: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
  batch_size: &lt;span class=&quot;token number&quot;&gt;64&lt;/span&gt;
tokenizer:
  tokenizer_name: &lt;span class=&quot;token variable&quot;&gt;${model.language_model.pretrained_model_name}&lt;/span&gt;
  vocab_file: null
  tokenizer_model: null
  special_tokens: null
language_model:
  pretrained_model_name: bert-base-uncased
  lm_checkpoint: null
  config_file: null
  config: null
head:
  num_fc_layers: &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;
  fc_dropout: &lt;span class=&quot;token number&quot;&gt;0.5&lt;/span&gt;
  activation: relu
  use_transformer_init: &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
optim:
  name: adam
  lr: &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;.0e-05
  weight_decay: &lt;span class=&quot;token number&quot;&gt;0.0&lt;/span&gt;
  sched:
    name: WarmupAnnealing
    warmup_steps: null
    warmup_ratio: &lt;span class=&quot;token number&quot;&gt;0.1&lt;/span&gt;
    last_epoch: &lt;span class=&quot;token parameter variable&quot;&gt;-1&lt;/span&gt;
    monitor: val_loss
    reduce_on_plateau: &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;download-domain-specific-pre-trained-models&quot;&gt;Download domain-specific pre-trained models&lt;/h2&gt;
</content>
  </entry>
  <entry>
    <title>Domain specific Token Classification Model</title>
    <link href="https://example.com/blog/domain-token-classification-model/" />
    <updated>2026-01-21T00:00:00Z</updated>
    <id>https://example.com/blog/domain-token-classification-model/</id>
    <content type="html">&lt;h2 id=&quot;domain-specific-token-classification-model&quot;&gt;Domain specific token classification model&lt;/h2&gt;
&lt;p&gt;In the following lines, we will see how to fine-tine a pre-trained language model to perform token classification for specific domains.
We will develop an NER model that finds disease names in medical disease abstracts.&lt;/p&gt;
&lt;p&gt;We are going to use the NCBI dataset, which contains set of 793 PubMed abstracts, annotated by 14 annotators.&lt;/p&gt;
&lt;h3 id=&quot;download-data&quot;&gt;Download Data&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;import os
import wget

# set data path
DATA_DIR = &amp;quot;data/NCBI&amp;quot;
os.makedirs(DATA_DIR, exist_ok=True)

with open(f&#39;{DATA_DIR}/NCBI_corpus_testing.txt&#39;) as f: 
    sample_text=f.readline()
    
print(sample_text)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows how the NCBI dataset looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;9288106	Clustering of missense mutations in the &amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;ataxia-telangiectasia&amp;lt;/category&amp;gt; gene in a &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;sporadic T-cell leukaemia&amp;lt;/category&amp;gt;.	&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;Ataxia-telangiectasia&amp;lt;/category&amp;gt; ( &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt; ) is a &amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;recessive multi-system disorder&amp;lt;/category&amp;gt; caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of &amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;cancer&amp;lt;/category&amp;gt; , especially &amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;lymphoid neoplasias&amp;lt;/category&amp;gt; , is substantially elevated in &amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt; patients and has long been associated with chromosomal instability . By analysing &amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;tumour&amp;lt;/category&amp;gt; DNA from patients with &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;sporadic T-cell prolymphocytic leukaemia&amp;lt;/category&amp;gt; ( &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;T-PLL&amp;lt;/category&amp;gt; ) , a rare &amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;clonal malignancy&amp;lt;/category&amp;gt; with similarities to a &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;mature T-cell leukaemia&amp;lt;/category&amp;gt; seen in &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt; , we demonstrate a high frequency of ATM mutations in &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;T-PLL&amp;lt;/category&amp;gt; . In marked contrast to the ATM mutation pattern in &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt; , the most frequent nucleotide changes in this &amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;leukaemia&amp;lt;/category&amp;gt; were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;T-PLL&amp;lt;/category&amp;gt; samples had a previously reported &amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt; allele . In contrast , no mutations were detected in the p53 gene , suggesting that this &amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;tumour&amp;lt;/category&amp;gt; suppressor is not frequently altered in this &amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;leukaemia&amp;lt;/category&amp;gt; . Occasional missense mutations in ATM were also found in &amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;tumour&amp;lt;/category&amp;gt; DNA from patients with &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;B-cell non-Hodgkins lymphomas&amp;lt;/category&amp;gt; ( &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;B-NHL&amp;lt;/category&amp;gt; ) and a &amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;B-NHL&amp;lt;/category&amp;gt; cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated &amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;tumours&amp;lt;/category&amp;gt; establishes somatic inactivation of this gene in the pathogenesis of &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;sporadic T-PLL&amp;lt;/category&amp;gt; and suggests that ATM acts as a &amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;tumour&amp;lt;/category&amp;gt; suppressor . As constitutional DNA was not available , a putative hereditary predisposition to &amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;T-PLL&amp;lt;/category&amp;gt; will require further investigation . . 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;import re

# use regular expression to find labels
categories=re.findall(&#39;&amp;lt;category.*?&amp;lt;&#92;/category&amp;gt;&#39;, sample_text)
for sample in categories: 
    print(sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;ataxia-telangiectasia&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;sporadic T-cell leukaemia&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;Ataxia-telangiectasia&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;recessive multi-system disorder&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;cancer&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;lymphoid neoplasias&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;tumour&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;sporadic T-cell prolymphocytic leukaemia&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;T-PLL&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;clonal malignancy&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;mature T-cell leukaemia&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;T-PLL&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;leukaemia&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;T-PLL&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;A-T&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;tumour&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;leukaemia&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;tumour&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;B-cell non-Hodgkins lymphomas&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;B-NHL&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;B-NHL&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;DiseaseClass&amp;quot;&amp;gt;tumours&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;sporadic T-PLL&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;Modifier&amp;quot;&amp;gt;tumour&amp;lt;/category&amp;gt;
&amp;lt;category=&amp;quot;SpecificDisease&amp;quot;&amp;gt;T-PLL&amp;lt;/category&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the following code, we see that the abstract has been broken into sentences. Each sentence is further parsed into words with labels that correspond to the original HTML-styled tags in the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NER_DATA_DIR = f&#39;{DATA_DIR}/NER&#39;
os.makedirs(os.path.join(DATA_DIR, &#39;NER&#39;), exist_ok=True)

# show downloaded files
!ls -lh $NER_DATA_DIR

!head $NER_DATA_DIR/train.tsv
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output&quot;&gt;output&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;Identification	O
of	O
APC2	O
,	O
a	O
homologue	O
of	O
the	O
adenomatous	B-Disease
polyposis	I-Disease
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;preprocess-data&quot;&gt;Preprocess Data&lt;/h2&gt;
&lt;p&gt;We need to convert these to a format that is compatible with NeMo token classification module.&lt;/p&gt;
&lt;p&gt;Script for conversion can be found &lt;a href=&quot;https://github.com/NVIDIA-NeMo/NeMo/blob/stable/examples/nlp/token_classification/data/import_from_iob_format.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&quot;language-bash&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# invoke the conversion script &lt;/span&gt;
&lt;span class=&quot;token operator&quot;&gt;!&lt;/span&gt;python import_from_iob_format.py &lt;span class=&quot;token parameter variable&quot;&gt;--data_file&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token variable&quot;&gt;$NER_DATA_DIR&lt;/span&gt;/train.tsv
&lt;span class=&quot;token operator&quot;&gt;!&lt;/span&gt;python import_from_iob_format.py &lt;span class=&quot;token parameter variable&quot;&gt;--data_file&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token variable&quot;&gt;$NER_DATA_DIR&lt;/span&gt;/dev.tsv
&lt;span class=&quot;token operator&quot;&gt;!&lt;/span&gt;python import_from_iob_format.py &lt;span class=&quot;token parameter variable&quot;&gt;--data_file&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token variable&quot;&gt;$NER_DATA_DIR&lt;/span&gt;/test.tsv

&lt;span class=&quot;token comment&quot;&gt;# preview dataset&lt;/span&gt;
&lt;span class=&quot;token operator&quot;&gt;!&lt;/span&gt;head &lt;span class=&quot;token parameter variable&quot;&gt;-n&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;token variable&quot;&gt;$NER_DATA_DIR&lt;/span&gt;/text_train.txt
&lt;span class=&quot;token operator&quot;&gt;!&lt;/span&gt;head &lt;span class=&quot;token parameter variable&quot;&gt;-n&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;token variable&quot;&gt;$NER_DATA_DIR&lt;/span&gt;/labels_train.txt&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&quot;output-2&quot;&gt;output&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor . 
O O O O O O O O B-Disease I-Disease I-Disease I-Disease O O 
&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
  <entry>
    <title>Microsoft 365 Copilot Chat Explorer Learning Path</title>
    <link href="https://example.com/blog/microsoft-copilot-learning/" />
    <updated>2026-01-20T00:00:00Z</updated>
    <id>https://example.com/blog/microsoft-copilot-learning/</id>
    <content type="html">&lt;h1 id=&quot;microsoft-365-copilot-chat-explorer-learning-path-completed&quot;&gt;Microsoft 365 Copilot Chat Explorer — Learning Path Completed&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;By Sumit Prasad — Published Jan 16, 2026&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Completed the Microsoft 365: Copilot Chat Explorer learning path on Microsoft Learn. Let’s see the following points covered in the course, including Copilot Chat functionality, smart prompting techniques, and productivity-driven use cases.&lt;/p&gt;
&lt;h2 id=&quot;introduction-to-microsoft-365-copilot-chat&quot;&gt;Introduction to Microsoft 365 Copilot Chat&lt;/h2&gt;
&lt;p&gt;Microsoft 365 Copilot Chat is a secure AI assistant for work. It is available with Microsoft 365 subscription at no additional cost. It is designed to serve as a powerful AI assistant to streamline work and boost productivity.&lt;/p&gt;
&lt;p&gt;It helps you with tasks such as summarizing, brainstorming, writing, coding and searching. It also provides intelligent suggestions and automates repetitive tasks.&lt;/p&gt;
&lt;p&gt;The Copilot Chat includes the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is secure and powered by the latest advanced model (GPT-5) offered by OpenAI.&lt;/li&gt;
&lt;li&gt;IT teams can control enterprise data protection and agent management.&lt;/li&gt;
&lt;li&gt;It includes features like Copilot pages, file upload and image generation (a SharePoint license might be required to use Pages).&lt;/li&gt;
&lt;li&gt;It adheres to Microsoft’s AI principles.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Copilot Chat uses the Microsoft Graph to find and pull together information to give helpful, work-related answers. It automatically selects the right model based on context to resolve issues more quickly for simple problems and with deeper reasoning for complex ones.&lt;/p&gt;
&lt;p&gt;The Copilot Chat can be accessed by visiting &lt;strong&gt;m365copilot.com&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Copilot Chat is enabled in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Teams (web, Windows, Mac, mobile)&lt;/li&gt;
&lt;li&gt;Outlook (web, Windows, Mac, mobile)&lt;/li&gt;
&lt;li&gt;Word, PowerPoint, Excel, Microsoft Edge browser and OneNote via a side pane&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;writing-effective-prompts&quot;&gt;Writing Effective Prompts&lt;/h2&gt;
&lt;p&gt;Writing effective prompts is a great way to perform tasks in Copilot Chat such as creating, summarizing, editing or transforming content.&lt;/p&gt;
&lt;p&gt;The four important key elements for effective prompts are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;br&gt;
Tell the chatbot &lt;em&gt;why&lt;/em&gt; you need something and &lt;em&gt;who&lt;/em&gt; is involved. Provide detailed context and specifics for better responses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;br&gt;
Tell the chatbot &lt;em&gt;what&lt;/em&gt; you want it to do. Including examples of output can help shape the response.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;&lt;br&gt;
Tell the chatbot &lt;em&gt;where&lt;/em&gt; to look for information. This can include files you upload or public URLs you want it to reference.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Expectations&lt;/strong&gt;&lt;br&gt;
Tell the chatbot &lt;em&gt;how&lt;/em&gt; to deliver the answer—tone, style, length, and audience.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Prompt:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&amp;quot;I work in marketing focusing on competitor research. Give me a concise summary of recent news about Microsoft, using web articles from the last 2 months. Provide the answer in two to three paragraphs and use a business tone.&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;do-s-and-don-ts-of-prompting&quot;&gt;Do’s and Don’ts of Prompting&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do’s&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide specific instructions about topic, purpose, and output style&lt;/li&gt;
&lt;li&gt;Frame prompts as a natural conversation&lt;/li&gt;
&lt;li&gt;Occasionally give feedback to refine responses&lt;/li&gt;
&lt;li&gt;Use correct punctuation and grammar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Don’ts&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoid vague language&lt;/li&gt;
&lt;li&gt;Avoid changing topics abruptly&lt;/li&gt;
&lt;li&gt;Do not request inappropriate or unethical content&lt;/li&gt;
&lt;li&gt;Do not interrupt during answer generation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Copilot Chat also offers a &lt;em&gt;Prompt Gallery&lt;/em&gt; where users can use pre-designed prompts, save favorites, edit existing ones, or share prompts with team members.&lt;/p&gt;
&lt;h2 id=&quot;common-use-cases-with-copilot-chat&quot;&gt;Common Use-Cases with Copilot Chat&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Summarizing large amounts of data into clear insights&lt;/li&gt;
&lt;li&gt;Brainstorming ideas, drafting or refining content&lt;/li&gt;
&lt;li&gt;Analyzing uploaded files to summarize or generate content&lt;/li&gt;
&lt;li&gt;Generating AI imagery for projects or social posts&lt;/li&gt;
&lt;li&gt;Writing or checking code quality and performing data analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;using-copilot-chat-in-microsoft-365-apps&quot;&gt;Using Copilot Chat in Microsoft 365 Apps&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Outlook&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask questions about emails or attachments&lt;/li&gt;
&lt;li&gt;Rewrite message replies in different tones&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quickly create, edit, or refine content&lt;/li&gt;
&lt;li&gt;Ask questions about parts of documents for summaries or rewrites&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;PowerPoint&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Summarize slides&lt;/li&gt;
&lt;li&gt;Generate images from prompts&lt;/li&gt;
&lt;li&gt;Provide content ideas&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Excel&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask questions to analyze data trends and patterns&lt;/li&gt;
&lt;li&gt;Quickly generate visualizations&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>This is my fourth post</title>
    <link href="https://example.com/blog/fourthpost/" />
    <updated>2018-09-30T00:00:00Z</updated>
    <id>https://example.com/blog/fourthpost/</id>
    <content type="html">&lt;p&gt;Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.&lt;/p&gt;
&lt;p&gt;Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.&lt;/p&gt;
&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://example.com/blog/fourthpost/xbVnp_hAou-350.avif 350w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://example.com/blog/fourthpost/xbVnp_hAou-350.webp 350w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://example.com/blog/fourthpost/xbVnp_hAou-350.png&quot; alt=&quot;A possum parent and two possum kids hanging from the iconic red balloon&quot; width=&quot;350&quot; height=&quot;685&quot;&gt;&lt;/picture&gt;
&lt;h2 id=&quot;section-header&quot;&gt;Section Header&lt;/h2&gt;
&lt;p&gt;Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>This is my first post.</title>
    <link href="https://example.com/blog/firstpost/" />
    <updated>2018-05-01T00:00:00Z</updated>
    <id>https://example.com/blog/firstpost/</id>
    <content type="html">&lt;p&gt;Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.&lt;/p&gt;
&lt;p&gt;Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.&lt;/p&gt;
&lt;h2 id=&quot;section-header&quot;&gt;Section Header&lt;/h2&gt;
&lt;p&gt;Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.&lt;/p&gt;
&lt;pre class=&quot;language-diff-js&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-diff-js&quot;&gt;&lt;span class=&quot;token unchanged language-js&quot;&gt;&lt;span class=&quot;token prefix unchanged&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token comment&quot;&gt;// this is a command&lt;/span&gt;
&lt;span class=&quot;token prefix unchanged&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;myCommand&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;span class=&quot;token inserted-sign inserted language-js&quot;&gt;&lt;span class=&quot;token prefix inserted&quot;&gt;+&lt;/span&gt;  &lt;span class=&quot;token keyword&quot;&gt;let&lt;/span&gt; counter &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;span class=&quot;token deleted-sign deleted language-js&quot;&gt;&lt;span class=&quot;token prefix deleted&quot;&gt;-&lt;/span&gt;  &lt;span class=&quot;token keyword&quot;&gt;let&lt;/span&gt; counter &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;span class=&quot;token unchanged language-js&quot;&gt;&lt;span class=&quot;token prefix unchanged&quot;&gt; &lt;/span&gt;  counter&lt;span class=&quot;token operator&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token prefix unchanged&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
&lt;/span&gt;
&lt;span class=&quot;token unchanged language-js&quot;&gt;&lt;span class=&quot;token prefix unchanged&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token comment&quot;&gt;// Test with a line break above this line.&lt;/span&gt;
&lt;span class=&quot;token prefix unchanged&quot;&gt; &lt;/span&gt;console&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&#39;Test&#39;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
</feed>